{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naturalistic Spatial Prediction class: \u001b[97mInitialised\u001b[0m\n",
      "\n",
      "Class contains the following attributes:\n",
      "\u001b[34m .analyse\u001b[0m\n",
      "\u001b[34m .attributes\u001b[0m\n",
      "\u001b[34m .cortex\u001b[0m\n",
      "\u001b[34m .datafetch\u001b[0m\n",
      "\u001b[34m .explore\u001b[0m\n",
      "\u001b[34m .hidden_methods\u001b[0m\n",
      "\u001b[34m .initialise\u001b[0m\n",
      "\u001b[34m .nsd_datapath\u001b[0m\n",
      "\u001b[34m .own_datapath\u001b[0m\n",
      "\u001b[34m .stimuli\u001b[0m\n",
      "\u001b[34m .subjects\u001b[0m\n",
      "\u001b[34m .utils\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# This script pulls the feature maps from the specified layer of the CNN for each subject runs\n",
    "# dimensionality reduction on them using incremental PCA. Can take a while and can be adapted\n",
    "\n",
    "import os\n",
    "# conda\n",
    "# Limit the number of CPUs used to 2\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\" # For layer 0 and 2 try to limit it to 1, so that there is no multi-threading issue\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.feature_extraction import (\n",
    "    create_feature_extractor,\n",
    "    get_graph_node_names,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from torchvision import models\n",
    "from typing import Dict, Tuple, Union, Optional\n",
    "\n",
    "os.chdir(\"/home/rfpred\")\n",
    "sys.path.append(\"/home/rfpred/\")\n",
    "sys.path.append(\"/home/rfpred/envs/rfenv/lib/python3.11/site-packages/\")\n",
    "sys.path.append(\"/home/rfpred/envs/rfenv/lib/python3.11/site-packages/nsdcode\")\n",
    "\n",
    "from classes.natspatpred import NatSpatPred\n",
    "NSP = NatSpatPred()\n",
    "NSP.initialise()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argparse arguments\n",
    "pca_fit_batch = 1000\n",
    "n_comps = 1000\n",
    "cnn_layer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rfpred/envs/rfenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rfpred/envs/rfenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prf_region = \"center_strict\"\n",
    "\n",
    "# Load the pretrained AlexNet model\n",
    "# model = models.vgg16_bn(pretrained=True)\n",
    "model = models.vgg16(pretrained=True)\n",
    "modeltype = model._get_name()\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_ids, transform=None, crop: bool = True):\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "        self.crop = crop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        if self.crop:\n",
    "            imgnp = NSP.stimuli.show_stim(img_no=img_id, hide=True, small=True, crop=False)[0][\n",
    "                163:263, 163:263\n",
    "            ]  # I CROP THEM, YOU SEE\n",
    "        else:\n",
    "            imgnp = NSP.stimuli.show_stim(img_no=img_id, hide=True, small=True, crop=False)[0]\n",
    "\n",
    "        imgPIL = Image.fromarray(imgnp)  # Convert into PIL from np\n",
    "\n",
    "        if self.transform:\n",
    "            imgPIL = self.transform(imgPIL)\n",
    "\n",
    "        return imgPIL\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # resize the images to 224x24 pixels\n",
    "        transforms.ToTensor(),  # convert the images to a PyTorch tensor\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        ),  # normalize the images color channels\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Module(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Module(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_ices = [0, 3, 6,]\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[\"classifier.6\"]) # Here the layer is specified !!!!\n",
    "\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)\n",
    "\n",
    "this_layer = train_nodes[cnn_layer + 1] #if cnn_layer != \"norm\" else \"x\"\n",
    "\n",
    "# Which layer to extract the features from # Also add this as argparse thing.\n",
    "# model_layer = \"features.2\" #@param [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\", \"classifier.2\", \"classifier.5\", \"classifier.6\"] {allow-input: true}\n",
    "\n",
    "# if cnn_layer != \"norm\":\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[this_layer]) # Here the layer is specified !!!!\n",
    "\n",
    "train_batch = pca_fit_batch\n",
    "apply_batch = 500  # The image batch over which the fitted PCA is applied later on.\n",
    "fixed_n_comps = n_comps\n",
    "crop_imgs = True #IMPORTANT!!!!!!!!!!\n",
    "\n",
    "# image_ids = get_imgs_designmx()[subject][start:end] # This was for subject-specific image indices. Current line (below) is for all images.\n",
    "image_ids = list(range(0, train_batch))\n",
    "dataset = ImageDataset(image_ids, transform=preprocess, crop=False) # CHECK THIS CROP ARG\n",
    "dataloader = DataLoader(dataset, batch_size=train_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for extracting features, and fitting the pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(feature_extractor, dataloader, pca, cnn_layer: int|str):\n",
    "    while True:  # Keep trying until successful\n",
    "        try:\n",
    "            features = []\n",
    "            for i, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "                ft = feature_extractor(d)\n",
    "                # Flatten the features\n",
    "                ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "\n",
    "                # Print out some summary statistics of the features\n",
    "                print(\n",
    "                    f\"AlexNet layer: {cnn_layer}, Mean: {ft.mean()}, Std: {ft.std()}, Min: {ft.min()}, Max: {ft.max()}\"\n",
    "                )\n",
    "\n",
    "                # Check if the features contain NaN values\n",
    "                if np.isnan(ft.detach().numpy()).any():\n",
    "                    raise ValueError(\"NaN value detected\")\n",
    "\n",
    "                # Check for extreme outliers\n",
    "                if (ft.detach().numpy() < -100000).any() or (\n",
    "                    ft.detach().numpy() > 100000\n",
    "                ).any():\n",
    "                    raise ValueError(\"Extreme outlier detected before PCA fit\")\n",
    "\n",
    "                # Apply PCA transform\n",
    "                ft = pca.transform(ft.cpu().detach().numpy())\n",
    "                features.append(ft)\n",
    "            return np.vstack(features)  # Return the features\n",
    "        except ValueError as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            print(\"Restarting feature extraction...\")\n",
    "\n",
    "\n",
    "\n",
    "def extract_features_and_check(d, feature_extractor, cnn_layer):\n",
    "    while True:  # Keep trying until successful\n",
    "        try:\n",
    "            \n",
    "            # Extract features\n",
    "            ft = feature_extractor(d)\n",
    "            # Flatten the features\n",
    "            ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "\n",
    "            # Check for NaN values\n",
    "            if np.isnan(ft.detach().numpy().any()):\n",
    "                raise ValueError(\"NaN value detected before PCA fit\")\n",
    "\n",
    "            # Check for extreme outliers\n",
    "            if (ft.detach().numpy() < -100000).any() or (ft.detach().numpy() > 100000).any():\n",
    "                raise ValueError(\"Extreme outlier detected before PCA fit\")\n",
    "\n",
    "            return ft  # If everything is fine, return the features\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            print(\"Restarting feature extraction...\")\n",
    "\n",
    "\n",
    "def fit_pca(\n",
    "    feature_extractor,\n",
    "    dataloader,\n",
    "    pca_save_path=None,\n",
    "    fixed_n_comps: Optional[int] = None,\n",
    "    train_batch: int = None,\n",
    "    cnn_layer: int|str = None,\n",
    "):\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=None, batch_size=train_batch)\n",
    "\n",
    "    try:\n",
    "        if fixed_n_comps is None:\n",
    "            # Fit PCA to batch to determine number of components\n",
    "            print(\n",
    "                \"Determining the number of components to maintain 95% of the variance...\"\n",
    "            )\n",
    "            for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                ft = extract_features_and_check(d, feature_extractor, cnn_layer)\n",
    "                # Fit PCA to batch\n",
    "                pca.partial_fit(ft.detach().cpu().numpy())\n",
    "\n",
    "            # Calculate cumulative explained variance ratio\n",
    "            cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "            # Find the number of components to maintain 95% of the variance\n",
    "            n_comps = np.argmax(cumulative_var_ratio >= 0.95) + 1\n",
    "            print(f\"Number of components to maintain 95% of the variance: {n_comps}\")\n",
    "\n",
    "        else:\n",
    "            n_comps = fixed_n_comps\n",
    "            print(f\"Using fixed number of components: {n_comps}\")\n",
    "\n",
    "        # Set the number of components\n",
    "        pca = IncrementalPCA(n_components=n_comps, batch_size=train_batch)\n",
    "\n",
    "        # Fit PCA to the entire dataset\n",
    "        print(\"Fitting PCA with determined number of PCs to batch...\")\n",
    "        for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            ft = extract_features_and_check(d, feature_extractor, cnn_layer) # cnn_layer arg not used in function\n",
    "            # Fit PCA to batch\n",
    "            pca.partial_fit(ft.detach().cpu().numpy())\n",
    "\n",
    "        # Save the fitted PCA object if specified\n",
    "        if pca_save_path:\n",
    "            print(f\"Saving fitted PCA object to: {pca_save_path}\")\n",
    "            joblib.dump(pca, pca_save_path)\n",
    "\n",
    "        # Return the fitted PCA object\n",
    "        print(\"PCA fitting completed.\")\n",
    "        return pca\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(\"PCA fitting failed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean argument to include dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_str = \"dense/\" if dense else \"\"\n",
    "os.makedirs(f\"{NSP.own_datapath}/visfeats/cnn_featmaps/{modeltype}/{dense_str}\", exist_ok=True)\n",
    "\n",
    "smallpatch_str = \"smallpatch_\" if crop_imgs else \"\"\n",
    "\n",
    "# Fit PCA and get the fitted PCA object\n",
    "pca = fit_pca(\n",
    "    feature_extractor,\n",
    "    dataloader,\n",
    "    # pca_save_path=f\"/home/rfpred/data/custom_files/visfeats/cnn_featmaps/pca_{cnn_layer}_{fixed_n_comps}pcs.joblib\",\n",
    "    pca_save_path=f\"{NSP.own_datapath}/visfeats/cnn_featmaps/{modeltype}/{dense_str}pca_{smallpatch_str}{cnn_layer}_{fixed_n_comps}pcs.joblib\",\n",
    "    fixed_n_comps=fixed_n_comps,\n",
    "    train_batch=train_batch,\n",
    "    cnn_layer=cnn_layer,\n",
    "    )\n",
    "\n",
    "del dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the dataset and dataloader with the entire image set to apply the fitted PCA to.\n",
    "all_img_ids = list(range(0, 73000))  # All the NSD images\n",
    "# all_img_ids = list(NSP.stimuli.imgs_designmx()[\"subj01\"]) # If it still is too heavy\n",
    "full_dataset = ImageDataset(all_img_ids, transform=preprocess, crop=False)\n",
    "full_dataloader = DataLoader(full_dataset, batch_size=apply_batch, shuffle=False)\n",
    "\n",
    "# Check if PCA fitting was successful\n",
    "if pca is not None:\n",
    "    # Apply the fitted PCA to the rest of the dataset\n",
    "    features_algo = extract_features(\n",
    "        feature_extractor, full_dataloader, pca, cnn_layer\n",
    "    )\n",
    "else:\n",
    "    print(\"PCA fitting failed. Unable to apply PCA, fock.\")\n",
    "\n",
    "# np.savez(\n",
    "#     # f\"/home/rfpred/data/custom_files/visfeats/cnn_featmaps/featmaps/featmaps_lay{this_layer}.npz\",\n",
    "#     f\"/home/rfpred/data/custom_files/visfeats/cnn_featmaps/featmaps/featmaps_smallpatch_lay{this_layer}.npz\",\n",
    "#     *features_algo,\n",
    "# )\n",
    "\n",
    "os.makedirs(f\"{NSP.own_datapath}/visfeats/cnn_featmaps/{modeltype}/featmaps/\", exist_ok=True)\n",
    "\n",
    "np.savez(\n",
    "    # f\"/home/rfpred/data/custom_files/visfeats/cnn_featmaps/featmaps/featmaps_lay{this_layer}.npz\",\n",
    "    f\"{NSP.own_datapath}/visfeats/cnn_featmaps/{modeltype}/featmaps/featmaps_{smallpatch_str}lay{this_layer}.npz\",\n",
    "    *features_algo,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
