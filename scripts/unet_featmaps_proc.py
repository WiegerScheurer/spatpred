#!/usr/bin/env python3

# This script is used to process the feature maps generated by the U-Net model for each layer.
# Dimensionality reduction is done using incrementalPCA and layer-specific stacks are saved.

import os
# Limit the number of CPUs used to 2
os.environ["OMP_NUM_THREADS"] = "5"

import os
import sys

import numpy as np
import random
import matplotlib.pyplot as plt
import pandas as pd
import glob
import pickle
import torch
from torchvision import transforms
from PIL import Image
import torchvision.models as models
from torch.nn import Module
# from matplotlib import colormaps
from sklearn.decomposition import PCA
from sklearn.decomposition import IncrementalPCA

os.chdir('/home/rfpred')
sys.path.append('/home/rfpred')
sys.path.append('/home/rfpred/envs/rfenv/lib/python3.11/site-packages/')
sys.path.append('/home/rfpred/envs/rfenv/lib/python3.11/site-packages/nsdcode')


# from funcs.imgproc import rand_img_list, show_stim, get_imgs_designmx

scale = 'cropped' # alternative: 'full'

cnnfeats = {}
for layer in range(1,5):
    cnnfeats[f'layer_{layer}'] = []

    # Get a list of all files that start with 'feats_gt_np'
    file_list = glob.glob(f'/home/rfpred/data/custom_files/subj01/pred/featmaps/{scale}/feats_gt_np*')

    # Define a custom sorting key that extracts the number following 'feats_gt_np_'
    def sorting_key(filename):
        base_name = filename.split('/')[-1]  # Get the filename without the directory
        number = base_name.split('_')[-1]  # Get the number following 'feats_gt_np_'
        number = number.split('.')[0]  # Remove the '.pkl' extension
        return int(number)

    # Sort the file list using the custom sorting key
    file_list = sorted(file_list, key=sorting_key)

    # Loop over the files and load each one
    for file in file_list:
        with open(file, 'rb') as f:
            array = pickle.load(f)[layer]
            cnnfeats[f'layer_{layer}'].extend(array)

# Works, but takes a LOT of time, about 30 mins
# Initialize an empty dictionary to store the transformed data for each layer
transformed_layers = {}

# Loop over the layers in the cnnfeats dictionary
for layer_name, layer in cnnfeats.items():
    print(f"Processing {layer_name}...")
    
    # Initialize an empty list to store the transformed data for each image
    transformed_images = []
    
    # Reshape the layer so that all feature maps are flattened into a single vector
    reshaped_layer = np.array(layer).reshape(len(layer)*len(layer[0]), -1)
    
    # Run an initial PCA to determine the number of components needed to explain at least 80% of the variance
    ipca = IncrementalPCA(n_components=None)
    ipca.fit(reshaped_layer)
    
    cumulative_explained_variance_ratio = np.cumsum(ipca.explained_variance_ratio_)
    n_components = np.argmax(cumulative_explained_variance_ratio >= 0.8) + 1
    
    # Loop over the images in the layer
    for i in range(len(layer)):
        if (i+1) % 100 == 0:
            print(f"\tProcessing image {i+1}/{len(layer)}...")
        
        # Select the current image
        image = layer[i]
        
        # Reshape the image so that all feature maps are flattened into a single vector
        reshaped_image = np.array(image).reshape(len(image), -1)
        
        # Run PCA with the determined number of components
        ipca = IncrementalPCA(n_components=n_components)
        transformed_image = ipca.fit_transform(reshaped_image)
        
        # Append the transformed image to the list
        transformed_images.append(transformed_image)
    
    # Append the list of transformed images for the layer to the transformed_layers dictionary
    transformed_layers[layer_name] = transformed_images
        
# This was used to saved the feature maps that have been reduced in dimensionality using PCA
# Initialize an empty list to store the reshaped data for each layer
reshaped_layers = []

# Save the transformed layers and flatten the last 2 dimensions to get a 2D array that can be used as X matrix
for layer in range(1,5):
    layer_data = np.array(transformed_layers[f'layer_{layer}'])
    n_imgs, _, _ = layer_data.shape
    reshaped_layer_data = layer_data.reshape(n_imgs, -1)
    np.save(f'/home/rfpred/data/custom_files/subj01/pred/featmaps/{scale}_unet_gt_feats_{layer}', reshaped_layer_data)
    reshaped_layers.append(reshaped_layer_data.flatten())
    
print("All is well")
